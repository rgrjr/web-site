<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML//EN">
<html>
<head>
<title>Bob Rogers:  System backups</title>
<link href="../site.css" title="Default" rel="stylesheet" type="text/css">
</head>

<body bgcolor=white>
<h1>System backup</h1>

<p><a href="/"><tt>Home</tt></a> : <a href="index.html">Linux
resources</a> : <a href="howto.html">"Howto"</a> : Backup
<hr>

<a name="abstract">
<h2>Abstract</h2>

<p>After describing <a href="#technologies">the difference between
archival and mirroring backups</a> and some common backup tools used for
each by Unix system administrators, I describe the general
considerations that go into setting up an <a href="#archival">archival
backup system</a>, and describe the <a href="#archival-tools">tools I
use for archival backups</a>.  I conclude with a briefer section of <a
href="#mirroring">notes on mirroring backup</a> -- briefer both because
the technology is simpler, and because I use it less.
</p>

<h2>Table of contents</h2>

<!-- hhmtoc start -->
<ol>
  <li> System backup
       <ol>
	 <li> <a href="#abstract">Abstract</a>
	 <li> Table of contents
	 <li> <a href="#technologies">Backup technologies</a>
	 <li> <a href="#archival">Notes on archival backups</a>
	      <ol>
		<li> <a href="#inc-levels">Backup levels</a>
		<li> <a href="#frequency">Backup frequency</a>
		<li> <a href="#timing">Backup timing</a>
		<li> <a href="#cron-backup">Automated backups with <tt>cron</tt></a>
	      </ol>
	 <li> <a href="#archival-tools">Tools for archival backup dumps</a>
	      <ol>
		<li> <a href="#tar-backups">Archival backups with <tt>tar</tt></a>
		<li> <a href="#dump-backups">Archival backups with <tt>dump</tt></a>
		<li> <a href="#dar-backups">Archival backups with <tt>dar</tt></a>
		<li> <a href="#backup-scripts">Backup tools in the "scripts" project</a>
		<li> <a href="#perl-script">The <tt>backup.pl</tt> Perl script</a>
		<li> <a href="#show-backups.pl">Listing backups with <tt>show-backups.pl</tt></a>
		<li> <a href="#vacuum.pl">Copying dump files with <tt>vacuum.pl</tt></a>
		<li> <a href="#clean-backups.pl">Tidying up with <tt>clean-backups.pl</tt></a>
	      </ol>
	 <li> <a href="#mirroring">Notes on mirroring backup</a>
	      <ol>
		<li> <a href="#web-mirror">Mirroring case history 1:  Web server content</a>
		<li> <a href="#disk-mirror">Mirroring case history 2:  Full disk copy</a>
	      </ol>
	 <li> <a href="#ack">Acknowledgements</a>
       </ol>
</ol>
<!-- hhmtoc end -->

<p>
<hr>
<a name="technologies">
<h2>Backup technologies</h2>

<p>The baseline motivation behind all backup systems is disaster
recovery: You want to ensure that your files will survive all hardware
failures that Murphy's Law might conceivably throw at you.  All backup
technologies meet this goal by making a copy, but there are really two
kinds of copies, with distinct recovery characteristics:  Archival, and
mirroring.

<p><b>Archival</b> backup gives you the ability to travel through time:
If you suddenly realize that an important file is missing, and you're
not sure when it was deleted, then the ability to sift through a year of
backup dumps looking for the missing file can be a life-saver.  In order
to do this, however, you must keep a lot of data around, and that almost
always means putting the backup dumps on some sort of offline storage.
</p>

<p><b>Mirroring</b> backup gives you immediate access to the most recent
copy of your data; if you deleted that important file just this morning,
then it's a snap to go get it from the backup drive, without any
searching.  On the other hand, if you deleted it before the last
mirroring operation, you are completely out of luck.  At a minimum,
mirroring only requires a spare disk of comparable size, and is easy to
automate completely, as it requires no manipulation of offline media.
</p>

<p>The "entry-level" backup options for Linux (and Unix systems
generally) tend to provide either archival or mirroring, but not both.
They are:
</p>

<ol>
  <li> <b>The standard Unix <tt>tar</tt> utility.</b> This is <a
       href="http://www.gnu.org/directory/GNU/tar.html"> GNU
       <tt>tar</tt></a> in free implementations (and even some other
       Unix flavors), and is the easiest tool for archiving particular
       directories.  It has the distinct advantage of being supremely
       portable; "tar" format can be read by all other Unix systems, and
       even by DOS/Windows and Macintosh.
       It is usually included in a default installation, but if not,
       <tt>"zypper install tar"</tt> will get it on openSUSE.
  </li>
  <li> <b>The traditional Unix <tt>dump</tt> and <tt>restore</tt>
       programs.</b> Most Linux systems come with the <a
       href="http://dump.sourceforge.net/"> <tt>dump/restore</tt>
       implementation for ext2/ext3</a>, but these are the traditional
       names for the archival backup programs in Unix, so
       <tt>"man&nbsp;dump"</tt> will almost always come up with
       something on any Unix system (try <tt>"zypper install dump"</tt>
       on openSUSE).
  </li>
  <li> <b>The <tt>dar</tt> program</b> is something between <tt>tar</tt>
       and <tt>dump</tt>; it stands for "disk archiver," as in
       disk-to-disk (do <tt>"zypper install dar"</tt> on openSUSE).
  </li>
  <li> <b>The <tt>rsync</tt> program.</b> Unlike the other three, <a
       href="http://rsync.samba.org/"> <tt>rsync</tt></a> is designed to
       mirror the content of directory trees over the network, is quite
       clever about only transferring data that have changed, and can
       also be set up to do local disk-to-disk copying.  (You guessed
       it, <tt>"zypper install rsync"</tt> on openSUSE).
  </li>
</ol>

<p>Fortunately, it is possible to have both archival and mirroring backup,
for those that need it.  For small to medium installations where high
availability is important, you can install a hybrid system where
archival dumps are created on a primary server, copied to a backup
server for safe-keeping, and also restored onto the backup server's
disks for quick access in the event that the primary server fails.
</p>

<p>And for many small installations, archival backups are sufficient.
This is all I need at home, in fact.
</p>

<p>It is also possible to do mirroring without archival, though I myself
would not recommend it.  But the low-maintenance of an <tt>rsync</tt>
solution may make it the most appealing for some -- just be clear that
you're giving up your "data history" when you pass on archival.
</p>

<p>
<hr>
<a name="archival">
<h2>Notes on archival backups</h2>

In order to reduce the amount of storage required for archival backups,
it is desirable to skip files that haven't changed since the last
backup.  Obviously, the first backup must contain everything, but a
series of subsequent backups need only contain the files changed since
the last backup; in the event of a disaster, restoring all backups in
the order in which they were made will return the file system to the
same state as if it had been restored from a single full backup made on
the last day.  This scheme still has two drawbacks:  The first is that
the process of restoring the file system gets to be quite tedious after
a few weeks, since there are quite a few of them at that point.  Worse
yet, data will be lost if any of those backups somehow gets lost or
becomes corrupted.

<a name="inc-levels">
<h3>Backup levels</h3>

<p>In order to address these drawbacks, it is useful to define a
<b>backup level</b> between 0 and 9 that controls how comprehensive to
make the backup.  Each level <i>k</i> backup contains a snapshot of all
files changed since the level <i>k-1</i> dump (or the dump made at the
next <i>lower</i> numeric level if there is no level <i>k-1</i> dump).
Level 0 is therefore the most comprehensive, and level 9 is the most
"incremental."  At this point, some additional terminology is in order:
<ul>
  <li> A <b>full backup dump</b> is a complete snapshot of the state of
       some portion of the filesystem at the time the backup was made.
       Full dumps are defined to be at level 0.
  <li> A <b>consolidated backup dump</b> is a snapshot made <i>of the
       same portion of the filesystem</i> at a later time that only
       contains those files that have changed since the last full dump.
       Consolidated dumps are always at level 1, and are usually made
       weekly.  When these get to be too large, then it's time for a new
       full dump.
  <li> <b>Incremental dumps</b> have levels between 2 and 9 inclusive,
       and are usually made daily.  A given incremental may or may not
       reach all the way back to the last consolidated backup, but it
       certainly won't reach any farther.
</ul>

<a name='tower-of-hanoi'>
<p>In order to reduce the number of incrementals required, one can use
the "modified Tower of Hanoi algorithm" described in the <tt>dump</tt>
manpage, which prescribes the following sequence of incremental dump
levels (after having made a full or consolidated dump):
<pre>
    3 2 5 4 7 6 9 8 9 9 ...
</pre>
These are for daily backups, which is the absolute minimum period for a
workgroup server in an office environment.  At the end of the week, a
consolidated dump is performed, and the daily cycle starts over again.
At this point, last week's incrementals could be thrown away, as they
are no longer needed for disaster recovery, but it's a good idea to keep
them around for at least a month in order to cover the "I didn't mean to
delete that" syndrome.

<p>In any case, this multilevel backup system turns out to be quite
effective in reducing the size of backups; even after a month, a
consolidated dump can be only about 20% of the size of the full dump,
and the daily incrementals only 3 to 5%.

<a name="frequency">
<h3>Backup frequency</h3>

Deciding how often to make backups requires making a tradeoff between
how many days of work you are willing to lose versus how much effort you
have to spend on performing each backup.  That is why a high degree of
automation is a great advantange; it costs essentially nothing to take
backups every day.  My automated system costs me only 5 to 10 minutes
per week, mostly to write consolidated backups to CD, and changing the
daily backup schedule wouldn't affect that at all.

<p>For less automated systems, the cost may be 5 to 10 minutes for each
backup dump.  A system failure that requires restoring from backups
could happen at any time during the backup cycle, which means that the
expected amount of work lost for each failure is half of the usage
between backup intervals.  In other words, if the system is backed up
after every 40 hours of use, then the expected loss due to backup
failure is 20 hours.  It seems reasonable to set the expected loss over
the course of a year equal to the planned time investment, and then
solve for the backup frequency in order to find a value that minimizes
expected total effort.  (Finding the true optimum probably isn't much
harder, but it's not clear that it's worth the effort.)  If we do that,
we get:

<blockquote>
<i>I*f = W*F/(2*f)</i><br>
<br>
<i>f<sup>2</sup> = W*F/(2*I)</i><br>
<br>
<i>f = sqrt(W*F/(2*I))</i>
</blockquote>

where

<ul>
  <li> <i>f</i> = backup frequency (i.e. number of backups per unit
       time).
  <li> <i>I</i> = time invested in performing each backup.
  <li> <i>W</i> = total system usage per unit time.
  <li> <i>F</i> = expected number of system failures per unit time.
</ul>

<p>Of course there are other costs to consider, such as inconvenience to
customers (and staff embarassment) when you have to admit that you lost
their emails, but these mostly define a "maximum acceptable loss"
ceiling, underneath which it is still desirable to seek an optimum.

<p>If there is only one user who uses the system for 40 hours per week,
and who does their own backups, then we have what might be called the
"standard home office scenario."  For this scenario, and assuming that
(a) backups take 10 minutes on average, and (b) the system is likely to
fail once per year on average (which might or might not be pessimistic),
then we arrive at the following optimal backup frequency for the home
office:

<blockquote>
<i>f<sub>opt</sub> = sqrt((120000 min/yr*1 failure/yr)/(2*10min))</i><br>
<br>
<i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = sqrt(6000) = 77.5 per yr</i>
</blockquote>

<p>This works out to be three times every two weeks, for a total time
investment (or expected time lost due to data recovery) of
<i>77.5*10&nbsp;=&nbsp;775</i> minutes, or about 13 hours.  We might
want to round this frequency to twice per week, then the time investment
is 1000 minutes (almost 17 hours!), and the expected time lost is only
10 hours (a quarter of a week).

<p>Most changes to this minimal scenario have the effect of driving the
ideal backup frequency up.  If there were ten people using the system
via file sharing, then the amount of potential lost work is ten times
higher, and so it becomes worth investing that 10 minutes every working
day (the actual optimal frequency is nearly 245 backups per year).  If
the time of the person making backups is only worth half as much as that
of the average file server user (in which case we should optimize the
dollar cost), then the "daily is optimal" point would be reached with
only 4 or 5 additional server users.  The end result is that it rarely
makes sense for small offices with shared file servers to do backups any
less often than daily.  If the resulting 41 hours per annum of staff
time spent on backups becomes excessive, then it's time to increase the
level of backup automation.

<a name="timing">
<h3>Backup timing</h3>

<p>Backup timing is also important, though often overlooked.  If the backup
system makes its copy of a given file while an application is partway
through updating it, the copy that winds up on the backup medium may be
inconsistent, and would appear to be corrupted to the application if it
were ever restored.  For this reason, it is best to make backups at
times when the file system isn't changing.  The middle of the night is
therefore ideal.
</p>

<p>Another solution to the "changing data during dump" problem is to
remount the filesystem read-only before performing the backup.  This has
never been practical for me; if the partition is exported via NFS (true
for all partitions I need to back up), I would need to unmount it on all
clients, possibly disrupting shell sessions or other long-running
processes.  The closest I've come is to edit <tt>/etc/fstab</tt> to mark
the partition as read-only temporarily and then reboot, but that doesn't
work for automated nightly backups, so I've only done it when making
extra just-in-case backups before server upgrades, when I am planning to
reboot the system anyway.
</p>

<p>A particularly nasty case of backup-induced corruption can be caused
by backing up the files used by a relational database management system
(RDBMS) to implement tables.  A transaction that updates multiple tables
may be in different stages of being written to disk for each table, so
the backup might be inconsistent even if it could be done
instantaneously.  There are really only two choices for archival backup
of a database:  Stop the RDBMS server completely (e.g. <tt>"systemctl
stop mariadb"</tt>) during the backup, or use a database client backup
program (e.g. <tt>mysqldump</tt> for the <a
href="http://www.mariadb.com/">MariaDB system</a>).  Doing the latter is
more robust, since it makes it more likely that old database content can
be restored into a much later version of the RDBMS system.
</p>

<p>For similar reasons, backing up more than once a day is probably not
worth the bother.  The only predictable period during the day when the
file system is highly unlikely to change is during the night when all
users are asleep.  And, for just those reasons, doing more than one
backup during this period would be pointless.
</p>

<a name='cron-backup'>
<h3>Automated backups with <tt>cron</tt></h3>

<p>A regular weekly schedule is easy to automate via cron jobs.  The
<tt>crontab</tt> entries for the full schedule for my <tt>/home</tt>
partition look like this:
</p>

<blockquote>
<pre>
# At 03:00 every night, do a /home backup.
00 03 * * Mon	/usr/local/bin/home-backups /dev/mapper/boot-home 3
00 03 * * Tue	/usr/local/bin/home-backups /dev/mapper/boot-home 2
00 03 * * Wed	/usr/local/bin/home-backups /dev/mapper/boot-home 5
00 03 * * Thu	/usr/local/bin/home-backups /dev/mapper/boot-home 4
00 03 * * Fri	/usr/local/bin/home-backups /dev/mapper/boot-home 7
00 03 * * Sat	/usr/local/bin/home-backups /dev/mapper/boot-home 6
00 03 * * Sun	/usr/local/bin/home-backups /dev/mapper/boot-home 1
# [full backup recipe.  -- rgr, 10-Apr-04.]
# 00 01 * * Mon	/usr/local/bin/home-backups /dev/mapper/boot-home 0
</pre>
</blockquote>

<p>When I first automated this process, I tried doing them daily, but
that got to be too much work, because I didn't change that many files,
and I still had to copy the backups to offline storage manually.  Making
the backup was no help from the point of view of disaster recovery if I
didn't copy it to another disk fairly promptly.  Consequently, I only
did the level 1, 2, 4, and 6 dumps in the <tt>crontab</tt> schedule
above.  Then I got a new desktop machine and set the old one up as a
server, which made it possible to copy all dumps automatically from the
server to the disk on the new machine.
</p>

<p>
<hr>
<a name="archival-tools">
<h2>Tools for archival backup dumps</h2>

<a name='tar-backups'>
<h3>Archival backups with <tt>tar</tt></h3>

<p>The traditional Unix <tt>tar</tt> program is not well suited to
making backups because it is only capable of creating full dumps.
However, the <a href="http://www.gnu.org/directory/GNU/tar.html"> GNU
<tt>tar</tt> program</a> can do incremental backup dumps, and it's
probably already installed on your GNU/Linux system, so it's worth
mentioning along with the other possiblities.
</p>

<p>To keep track of what's been dumped so far, <tt>tar</tt> uses what it
calls a "snapshot file," which is passed to the
<tt>--listed-incremental</tt> option.  Every time <tt>tar</tt> is run
with this option, it consults this file for the state of the filesystem
at the last backup, and updates it with files that it writes to the new
tarfile(s).  If a file is named that does not exist, then it is created,
and the resulting tarfile contains a full dump.  To keep the original
snapshot file from becoming modified (which makes subsequent
consolidated dumps impossible), the original snapshot must be copied for
each subsequent backup, so that each tarfile backup gets its own
snapshot, at least until it is superceded by a subsequent backup.
</p>

<p>The resulting backup protocol looks like this:
</p>

<ul>
  <li> For a full dump of <tt>/home</tt> made on 3-Jan-2021:
       <pre>
       # cd /home
       # tar --create --listed-incremental=/scratch/backups/home-20210103-l0.snap \
		--file=/scratch/backups/home-20210103-l0.tar.xz .
       # tar --diff --file=/root/home-20210103-l0.tar.xz
       #
</pre>
       This will create both files in <tt>/scratch/backups/</tt>.
       Adding the <tt>.xz</tt> suffix to the tarfile name automatically
       requests compression.
  </li>
  <li> For the next day's daily backup (note that 4-Jan-2021 was a
       Monday):
       <pre>
       # cd /home
       # cp home-20210103-l0.snap home-20210104-l3.snap
       # cd /scratch/backups
       # tar --create --listed-incremental=/scratch/backups/home-20210104-l3.snap \
		--file=/scratch/backups/home-20210104-l3.tar .
       # tar --diff --file=/root/home-20210104-l3.tar.xz
       #
</pre>
       And so on through the rest of the week, however in order to
       implement the <a href=#'tower-of-hanoi'>"modified tower of Hanoi"
       backup algoritm</a> on (e.g.) Tuesday 5-Jan-2021 we'll also want
       to copy Sunday's full-dump snapshot file.
  </li>
  <li> Finally, we get to Sunday's consolidated backup:
       <pre>
       # cd /home
       # cp home-20210103-l0.snap home-20210110-l1.snap
       # rm -f home-20210103-l{2|3|4|5|6|7}.snap
       # cd /scratch/backups
       # tar --create --listed-incremental=/scratch/backups/home-20210110-l1.snap \
		--file=/scratch/backups/home-20210110-l1.tar .
       # tar --diff --file=/root/home-20210110-l1.tar.xz
       #
</pre>
       We have gone back to <tt>home-20210103-l0.snap</tt> to get our
       consolidated dump, and we can get rid of the snapshots for the
       dailies, since we won't be making incrementals from them -- and
       likewise for previous consolidated dump snapshots, if there had
       been any.
</ul>

<p><i>[I am considering extending the <a
href="#perl-script"><tt>backup.pl</tt> Perl script</a> described below
to support GNU <tt>tar</tt> backups.  However, because GNU <tt>tar</tt>
requires keeping track of the additional snapshot file, it would require
extensive changes to the backup code infrastructure, and seems harder to
automate, so it's not clear that it's worth it.  -- rgr, 26-Jan-21.]</i>
</p>


<a name='dump-backups'>
<h3>Archival backups with <tt>dump</tt></h3>

<p>Historically, I used the standard, tried-and-true <a
href="http://dump.sourceforge.net/"> <tt>dump</tt> and
<tt>restore</tt></a> programs.  More recently I have used <tt>dar</tt>
for backups; it has some advantages and disadvantages over
<tt>dump</tt>; <a href="#dar-backups">see below</a> for details.
</p>

<p>An interesting characteristic of <tt>dump</tt> is that it accesses
the raw device file (i.e. <tt>/dev/hda5</tt>) of <tt>ext2</tt>,
<tt>ext3</tt>, and <tt>ext4</tt> file systems, instead of going through
the file system interface, which means that it can only work on whole
partitions, and it can miss file data that
is cached in in RAM and not yet written to disk.
The pros and cons of this approach are discussed on the <a
href="http://dump.sourceforge.net/isdumpdeprecated.html">"Is dump really
deprecated?"</a> page of the <a
href="http://dump.sourceforge.net/">Dump/restore utilities</a> project.
In a nutshell, it makes the "changing data during dump" problem worse,
though there are ways around this, but has the unique advantage that
partitions can be dumped without affecting any of the times recorded by
the file system.  And (as also mentioned in the <a
href="#timing">"Backup timing"</a> section) data that is changing during
the backup is only one of the tradeoffs you need to consider when
setting up a backup system.
</p>

<p><i>Note that the <a href="#perl-script"><tt>backup.pl</tt> Perl
script</a> described below used to support <tt>dump</tt> to create
backup files, but I dropped that support in 2017.  -- rgr,
25-Jan-21.]</i>
</p>

<p>To see how many bytes are likely to be written to a dump file, use
the <tt>"-S"</tt> option to <tt>dump</tt>, e.g.
</p>

<pre>
    dump -S2 /dev/hda9
</pre>

<p>for a level 2 dump of the <tt>/dev/hda9</tt> partition.
</p>

<a name="dar-backups">
<h3>Archival backups with <tt>dar</tt></h3>

"dar" stands for <a href="http://dar.linux.free.fr/">Disk ARchive</a>,
and has a number of advantages with respect to <tt>dump</tt>:

<ul>
  <li> <tt>dar</tt> can backup arbitrary directory trees, and not just
       ext* partitions.
  </li>
  <li> <tt>dar</tt>'s compression operates on individual files (like
       Zip), so an unreadable media block will not prevent recovery of
       all later files.
  </li>
  <li> <tt>dar</tt> manages multiple volumes (it calls them "slices")
       by itself, rather than requiring all volumes to be explicitly
       named.  (The <tt>backup.pl</tt> script used to take care of this,
       but I dropped <tt>dump</tt> support in 2017.)
  </li>
  <li> <tt>dar</tt> can create a "catalog" file to help it go directly
       to the right slice when restoring just a few files.  This is
       useful when restoring files off of DVD (but you still have to
       keep the first and last slice around).
  </li>
</ul>

<p>Unfortunately, there are also a few disadvantages:

<ul>
  <li> <tt>dar</tt> puts two dots in the file name; <tt>mkisofs</tt>
       turns the first one into an underscore when making an ISO9660
       image (though you can use its <tt>-relaxed-filenames</tt> option
       to get around that).
  </li>
  <li> <tt>dar</tt> doesn't have an equivalent to <tt>dump</tt>'s "-S"
       option for estimating backup sizes (though this is less of a
       problem now that disks have become ridiculously large).
  </li>
  <li> <tt>dar</tt> doesn't have a concept of "backup level"; to get an
       incremental dump, you must specify the previous dump set (or a
       catalog).  <tt>backup.pl</tt> handles this by finding the right
       dump, but you must still keep those dumps/catalogs online (also
       not a problem because of enormous disks).
  </li>
</ul>

<p>All in all, I find the drawbacks minor, and have come to prefer
<tt>dar</tt>; it has been my standard backup tool since 2008.
</p>

<a name='backup-scripts'>
<h3>Backup tools in the "scripts" project</h3>

<p>To assist in setting up a backup system, I have written a series of
Perl scripts that help to automate the tedious parts.  These are part of
the <a href="https://github.com/rgrjr/scripts/"> "scripts" project at
Github</a> and are available under an open-source license.
</p>

<ul>
  <li> The <a href="#perl-script"> <tt>backup.pl</tt></a> script creates
       backups that follow a standard naming convention (though it
       currently only supports <tt>dar</tt>).
  </li>
  <li> <a href="#show-backups.pl"><tt>show-backups.pl</tt></a> lists
       available backups in chronological order and identifies which
       ones are current.
  </li>
  <li> <a href="#vacuum.pl"><tt>vacuum.pl</tt></a> copies current
       backups them over the network.
  </li>
  <li> <a href="cd-burning.html#cd-dump.pl"><tt>cd-dump.pl</tt></a>
       writes them to DVD or CD media.
  </li>
  <li> <a href="#clean-backups.pl"><tt>clean-backups.pl</tt></a> removes
       daily backups to free up space once they become sufficiently old.
  </li>
</ul>

<p>With these tools, it is possible to automate the system backups of a
small to medium office site (up to 20 daily users) to a high degree.  A
<tt>cron</tt> job runs <tt>backup.pl</tt> to create backup files on the
primary server in a scratch (non-backed-up) partition which are then
copied by <tt>vacuum.pl</tt> in a second <tt>cron</tt> job to another
scratch partition on secondary system for safe keeping.  Once on the
secondary server, one can use <tt>cd-dump.pl</tt> to write the full and
consolidated dumps to offline media without loading the primary server.
Other <tt>cron</tt> jobs on each system can then run
<tt>clean-backups.pl</tt> periodically to remove the oldest daily
backups in order to preserve sufficient room for new backups.  Depending
on the amount of scratch disk space available and the volume of daily
file system churn, the sysadmin only needs to intervene a few times per
month to write offline media and to remove excess full and consolidated
dumps.
</p>

<a name="naming-convention">
<p>These tools use a backup file naming convention in order to make it
easier to keep track of what may amount to thousands of backup files
collected from multiple systems over many years.  All such files match
"<i>&lt;prefix&gt;</i>-<i>&lt;date&gt;</i>-l<i>&lt;level&gt;</i>.<i>&lt;slice&gt;</i>.dar"
for <tt>dar</tt> backups or
"<i>&lt;prefix&gt;</i>-<i>&lt;date&gt;</i>-<i>l&lt;level&gt;&lt;idx&gt;?</i>.dump"
for <tt>dump</tt> backups, where
</p>

<ul>
  <li> <i>&lt;prefix&gt;</i> is an alphanumeric prefix that identifies
       the backup fileset;
  </li>
  <li> <i>&lt;date&gt;</i> is an 8-digit ISO date such as "20210104";
  </li>
  <li> <i>&lt;level&gt;</i> is a single digit <a
       href="#inc-levels">backup level</a>; and
  </li>
  <li> the <i>&lt;slice&gt;</i> number or <i>&lt;idx&gt;</i> letter
       (required for <tt>dar</tt> but optional for <tt>dump</tt>)
       indicates the position in the series for multiple files that make
       up a single backup.
  </li>
</ul>

<p>To install "scripts" backup tools,
</p>

<pre>
    # git clone https://github.com/rgrjr/scripts
    # cd scripts
    # make install-backup
    ...
    #
</pre>

<p>By default, this will put the above scripts into
<tt>/usr/local/bin</tt> (along with <tt>backup-dbs.pl</tt> and
<tt>svn-dump.pl</tt>, which are not discussed here, as they are more
specialized), as well as intalling the classes they use where Perl can
find them.
</p>

<a name='perl-script'>
<h3>The <tt>backup.pl</tt> Perl script</h3>

<p>When <tt>backup.pl</tt> is run by root, it creates and verifies a set
of backup files using <a href="#dar-backups"> the <tt>dar</tt>
program</a>.  Usage is
</p>

<pre>
    backup.pl [--test] [--verbose] [--usage|-?] [--help]
	      [--date=&lt;string&gt;] [--name-prefix=&lt;string&gt;]
	      [--file-name=&lt;name&gt;]
	      [--dump-program=&lt;dump-prog&gt;]
	      [--gzip | -z] [--bzip2 | -y] [ --compression[=[algo:]level] ]
	      [--dest-dir=&lt;destination-dir&gt;] [--dump-dir=&lt;dest-dir&gt;]
	      [--volsize=&lt;max-vol-size&gt;]
	      [--mount-point=&lt;dir&gt; | &lt;dir&gt; ]
	      [--level=&lt;digit&gt; | &lt;level&gt;]
</pre>

<p>See the documentation in the script for argument descriptions, known
bugs, and other details.
</p>

<a name="show-backups.pl">
<h3>Listing backups with <tt>show-backups.pl</tt></h3>

<p>The <tt>show-backups.pl</tt> script lists all backup files it can
find under the search root(s) that follow <a
href="#naming-convention">the naming convention described above</a>.
The default search roots are any directories that match
<tt>"/scratch*/backups"</tt> but that can be overridden by specifying
directories on the command line.  Other options exist to constrain the
search by level, date, and prefix, and to modify the output format.
</p>

<p>Usage is as follows:
</p>

<pre>
    show-backups.pl [ --help ] [ --man ] [ --usage ] [ --prefix=&lt;pattern&gt; ... ]
		    [ --[no]slices ] [ --[no]date | --sort=(date|prefix|dvd) ]
		    [ --before=&lt;date&gt; ] [ --since=&lt;date&gt; ] [ --size-by-date ]
		    [ --level=&lt;level&gt; | --level=&lt;min&gt;:&lt;max&gt; ]
		    [ &lt;search-root&gt; ... ]

where:

    Parameter Name     Deflt  Explanation
     --before                 If specified, only dumps on or before this date.
     --help                   Print detailed help.
     --level            all   If specified, only do dumps in this range.
     --man                    Print man page.
     --prefix                 Partition prefix on files; may be repeated.
     --since                  If specified, only do dumps since this date.
     --size-by-date      no   Print a table of total size by dump date.
     --slices                 If specified, print only slice file names.
     --sort           prefix  Sort by prefix, date, or dvd order.
     --usage                  Print this synopsis.
</pre>

<p>Here is an example of the output:
</p>

<pre>
    # show-backups.pl --since 2021-1-1
     *    56890485 home-20210127-l5.1.dar [orion:/scratch/backups/]
     *    64615879 home-20210126-l2.1.dar [orion:/scratch/backups/]
	  57317283 home-20210125-l3.1.dar [orion:/scratch/backups/]
     *   106608760 home-20210124-l1.1.dar [orion:/scratch/backups/]
	  77181762 home-20210123-l6.1.dar [orion:/scratch/backups/]
	  69923104 home-20210122-l7.1.dar [orion:/scratch/backups/]
	  79237797 home-20210121-l4.1.dar [orion:/scratch/backups/]
	  46002295 home-20210120-l5.1.dar [orion:/scratch/backups/]
	  96332012 home-20210119-l2.1.dar [orion:/scratch/backups/]
	  78037401 home-20210118-l3.1.dar [orion:/scratch/backups/]
	  91735019 home-20210117-l1.1.dar [orion:/scratch/backups/]
	  97052916 home-20210116-l6.1.dar [orion:/scratch/backups/]
	  80359764 home-20210115-l7.1.dar [orion:/scratch/backups/]
	  98385074 home-20210114-l4.1.dar [orion:/scratch/backups/]
	  62601445 home-20210113-l5.1.dar [orion:/scratch/backups/]
	  73749317 home-20210112-l2.1.dar [orion:/scratch/backups/]
	  67564592 home-20210111-l3.1.dar [orion:/scratch/backups/]
	  96643821 home-20210110-l1.1.dar [orion:/scratch/backups/]
	 113825418 home-20210109-l6.1.dar [orion:/scratch/backups/]
	  92711331 home-20210108-l7.1.dar [orion:/scratch/backups/]
	 101974794 home-20210107-l4.1.dar [orion:/scratch/backups/]
	  72500168 home-20210106-l5.1.dar [orion:/scratch/backups/]
	  84543562 home-20210105-l2.1.dar [orion:/scratch/backups/]
     *    11687025 home-20210104-l0-cat.1.dar [orion:/scratch/backups/]
     *  1563907072 home-20210104-l0.1.dar [orion:/scratch/backups/]
     *    60423634 home-20210104-l0.2.dar [orion:/scratch/backups/]
	  73820512 home-20210102-l6.1.dar [orion:/scratch/backups/]
	  72067136 home-20210101-l7.1.dar [orion:/scratch/backups/]
    #
</pre>

<p>Note that the current backup files (the ones that would need to be
restored in order to recreate the most recent state of the filesystem)
are marked with a "*".
</p>

<a name="vacuum.pl">
<h3>Copying dump files with <tt>vacuum.pl</tt></h3>

<p><tt>vacuum.pl</tt> copies backup dump files from place to place,
being careful to copy only current backups, and checks for good copies
to guard against network corruption.  Usage for this is
</p>

<pre>
    vacuum.pl [--test] [--verbose] [--usage|-?] [--help]
	      [--from=&lt;source-dir&gt;] [--to=&lt;dest-dir&gt;]
	      [--mode=(mv|cp)] [--prefix=&lt;tag&gt; ... ]
	      [--since=&lt;date-string&gt;] [--min-free-left=&lt;size&gt;]
</pre>

<p>See the documentation in the script for argument descriptions, known
bugs, and other details.
</p>

<a name="clean-backups.pl">
<h3>Tidying up with <tt>clean-backups.pl</tt></h3>

<p>When following the <a href="#inc-levels">modified Tower of Hanoi
backup level scheme described above</a>, daily backups (those with a
backup level of 2 or greater) contain only one or two days worth of
data.  Odd dailies (3, 5, 7, and 9) have only one day, and even dailies
(2, 4, 6, and 8) have two days -- that day plus that of the previous odd
daily.  Consequently, it is less important to keep dailies around for
extended periods of time, so they can be deleted automatically when they
are no longer useful.  This is what <tt>clean-backups.pl</tt> does:
Maintains a specified minimum amount of free space on the filesystem
partition that is used for backups by removing first odd dailies that
are older than a specified threshold, starting with the oldest, and if
that does not restore enough space, it then removes even dailies.  No
output is generated except when <tt>clean-backups.pl</tt> fails to make
its quota, which makes it work well as a <tt>cron</tt> job; the sysadmin
gets an email when it's time to think about removing consolidated dumps.
</p>

<p>Full and consolidated backups cover much longer time periods, and are
usually kept around for much longer.  For this reason,
<tt>clean-backups.pl</tt> never deletes full or consolidated backups.
</p>

<p>Usage is as follows:
</p>

<pre>
            clean-backups.pl [ --conf=&lt;config-file> ]
                             [ --[no]test ] [ --verbose ... ]

            clean-backups.pl [ --usage | --help ]
</pre>

<p>The default configuration file is <tt>/etc/backup.conf</tt>, which
tells which partitions to clean, and how thoroughly to clean them.  Here
is an example:
</p>

<pre>
    # Backup configuration.

    [scorpio:/scratch]
    min-free-space = 10
    min-odd-retention = 60
    min-even-retention = 120
    clean = home

    [orion:/scratch]
    min-free-space = 0.5
    min-odd-retention = 60
    min-even-retention = 120
    clean = home
</pre>

<p>Notice that the same configuration file is shared between two
systems.  Minimum retention is specified as a number of days, and the
even and odd retention values are specified separately.  The
<tt>min-free-space</tt> is specified in GiB, and the <tt>clean</tt>
tells which prefixes should be cleaned. in the event that multiple
backup sets are stored on that partition.
</p>

<p>
<hr>
<a name="mirroring">
<h2>Notes on mirroring backup</h2>

<p>The weakness of mirroring backup is that it only gives you a single
archival time point from which to recover.  Of course, this assumes that
you only make a single mirrored copy; multiple copies could get quite
expensive, so it's not surprising that I've never heard of anyone who
has actually done multiple mirrored copies, except possibly for Web
content.
</p>

<p>The key parameter for mirroring backup is therefore the backup
frequency, which involves a tradeoff in the two different kinds of
recovery capability <a href="#technologies">discussed above</a>.  If you
back up more frequently, then you will lose less in the event of a
catastrophic failure (i.e. a disk crash), but you will also have less
time in which to recover from file corruption or accidental deletion.
The extremum of frequent backup is provided by
<a href="https://en.wikipedia.org/wiki/Standard_RAID_levels">
RAID 0</a>, in which backup
is transparent and so frequent as to be effectively instantaneous, and
recovery from single-disk failure is likewise transparent, but there is
no archival history whatsoever.  Having RAID is not the same as having a
backup!
</p>

<p>Another example of continuous mirroring backup is
<a href="https://en.wikipedia.org/wiki/Replication_(computing)#DATABASE"> 
database replication</a>.  In master-slave setup, a master database
server pushes changes to one or more slave servers; each server keeps a
copy of the data on its local disk(s), so that if the master server
fails, any of the slaves can be reconfigured to take over as the
replacement master.  However, the same caution applies:  Just because
your database is replicated doesn't mean that it's backed up!
</p>

<a name="web-mirror">
<h3>Mirroring case history 1:  Web server content</h3>

<p><a href="http://www.rgrjr.com">This Web server</a> uses <a
href="http://rsync.samba.org/"> <tt>rsync</tt></a> to mirror the <a
href="https://www.cons.org/cmucl/">CMU Common Lisp</a> download content
from <a href="https://common-lisp.net/">common-lisp.net</a>.  The server
runs the following <tt>cron</tt> job once a day as root:
</p>

<pre>
    cd /scratch/mirror
    rsync -avz common-lisp.net::project/cmucl/ cmucl > /root/cmucl-mirror-log.text
</pre>

<p>The <tt>-a</tt> switch requests archival copying; according to the
manual page, the <tt>-a</tt> option "... is a quick way of saying you
want recursion and want to preserve almost everything."  The <tt>-v</tt>
switch makes it verbose (which is low-cost, as the content has few but
very large files, and doesn't change often), and <tt>-z</tt> means to
use compression in transit.  This command contacts the
<tt>common-lisp.net</tt> <tt>rsync</tt> server and updates the contents
of the <tt>cmucl</tt> tree under <tt>/scratch/mirror/cmucl/</tt> on my
server -- without bothering to copy anything that hasn't been changed.
You can browse the content at <a
href="http://www.rgrjr.com/cmucl/downloads/">
<tt>http://www.rgrjr.com/cmucl/downloads/</tt></a>.
</p>

<a name="disk-mirror">
<h3>Mirroring case history 2:  Full disk copy</h3>

<p><tt>rsync</tt> can also be used for disk-to-disk copying within a
single system.  Here is how Anthony DiSante describes his backup system,
in which he uses <tt>rsync</tt> in lieu of archival backup:

<blockquote>
I use rsync for my weekly backups -- I've got two 120GB disks in my
computer, and I have a 250GB disk in an external firewire enclosure.
Once the external drive is mounted at <tt>/mnt/backup</tt>, 
all it takes is this simple command:

<pre>
    rsync -a --delete --exclude /mnt/backup / /mnt/backup
</pre>

The <tt>-a</tt> switch is for archival copying, <tt>--exclude</tt> tells
it not to copy the external drive onto itself, and <tt>--delete</tt>
means to delete any files on the destination that no longer exist on the
source.  The result is that, when complete, the disk at
<tt>/mnt/backup</tt> is an exact copy of my root filesystem (which
includes both 120GB disks).  <tt>rsync</tt> is of course known for its
highly efficient remote-update algorithm whereby only the changes in
files are transmitted; in practice, I find that my weekly backup takes
about an hour to run on my 172GB of used space.
</blockquote>

<p>Note that a system-to-system backup of this magnitude might not take
much longer; probably not much of that 172GB changes from week to week,
so <tt>rsync</tt> would figure that out and would only transfer the
differences.  Based on my experience, a full dump of 172GB
(uncompressed) would require 11 hours to transmit over a local 100BaseT
connection, so dealing with archival dumps of this size would be a pain.

<p>Also, since the backup drive is removed after update, this setup can
be extended to use two or more identically-configured external drives,
which are updated in rotation.  This requires no more effort than for a
single drive, but begins to provide some archival history, for those who
can afford the additional hardware.

<a name="ack">
<h2>Acknowledgements</h2>

Thanks to Anthony DiSante <tt>&lt;orders <i>at</i> nodivisions
<i>dot</i> com&gt;</tt> for pointing out that I had neglected to mention
<tt>rsync</tt>; the resulting reorganization of the material has made
this page much more comprehensive.

<p>
<hr>
<address><a href="/bob/contact.html">Bob Rogers
	<tt>&lt;rogers@rgrjr.dyndns.org&gt;</tt></a></address>
</body>
</html>
