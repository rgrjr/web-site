<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML//EN">
<html> <head>
<title>Bob Rogers:  System backups</title>
</head>

<body bgcolor=white>
<h2>System backup</h2>

<p><a href="/"><tt>Home</tt></a> : <a href="">Linux resources</a> : <a
href="howto.html">"Howto"</a> : Backup
<hr>

<h3>Table of contents</h3>

<!-- hhmtoc start -->
<ol>
  <li> System backup
       <ol>
	 <li> Table of contents
	 <li> <a href="#fs">A word about file systems</a>
	 <li> <a href="#dump-backups">Making backups with <tt>dump</tt></a>
	      <ol>
		<li> <a href="#perl-script">The <tt>backup.pl</tt> perl script</a>
		<li> <a href="#script">The <tt>backup</tt> shell script</a>
		<li> <a href="#strategy">Backup strategy</a>
		<li> <a href="#inc-levels">Backup levels</a>
		<li> <a href="#backup-log">Backup log file</a>
	      </ol>
	 <li> <a href="#tar-backups">Making <tt>tar</tt> backups</a>
	 <li> <a href="#mac-backup">Backing up Jan's Macintosh</a>
       </ol>
</ol>
<!-- hhmtoc end -->

<a name="fs">
<h3>A word about file systems</h3>

So far, I have only used the standard, tried-and-true <tt>ext2</tt> file
system.  I am beginning to have experience with <a
href="http://www.namesys.com/">ReiserFS</a>, which appears to be faster
and more reliable than <tt>ext2</tt>, but prefer not to use it on my
home system, because <a href="http://dump.sourceforge.net/">
<tt>dump</tt></a> doesn't understand ReiserFS partitions.  (I know
nothing about <tt>ext3</tt>, successor to <tt>ext2</tt>, except that
<tt>dump</tt> supports it.)

<a name='dump-backups'>
<h3>Making backups with <tt>dump</tt></h3>

<p>[need intro.  -- rgr, 8-Jan-03.]

<p>Making the backup is only half of the project; they also need to be
verified by using <tt>restore</tt> to compare the dump file contents
with the original disk data.  [this is not as essential as it was in the
bad old days, when media were much less reliable, and hardware was much
less clever about dealing with media failures.  -- rgr, 8-Jan-03.]

<p>Both of the following scripts (Perl and <tt>csh</tt>) call
<tt>dump</tt> to create backup files using the same naming convention,
and then verify the file using <tt>restore</tt>.  Initially, I wrote the
<a href="#script">shell script version</a> first, then recoded it into
Perl when I needed to add sufficient functionality that I found the
shell syntax too confining.  The <a href="#perl-script">
<tt>backup.pl</tt> Perl script</a> has a number of extra features, plus
changed defaults that make it more convenient to use for CD-ROM dumps.

<a name='perl-script'>
<h4>The <tt>backup.pl</tt> perl script</h4>

When <tt>backup.pl</tt> is run by root, it creates a <a
href="http://dump.sourceforge.net/"> <tt>dump</tt></a> format backup
file and verifies it using <tt>restore</tt>.  Usage for this is
<pre>
    backup.pl [--[no]cd] [--file-name=&lt;name&gt;] [--dump-dir=&lt;dest-dir&gt;]
              [--test] [--verbose] [--usage|-?] [--help] [--cd-dir=&lt;mv-dir&gt;]
              [--partition=&lt;block-special-device&gt;] [--level=&lt;digit&gt;]
              [--volsize=&lt;max-vol-size&gt;] [&lt;partition&gt;] [&lt;level&gt;]
</pre>
See <a href="backup.pl.html">the <tt>backup.pl</tt> <tt>man</tt>
page</a> for argument descriptions, and other details.

<p>For complete (level 0) dumps, as well as the larger level 1 dumps of
the <tt>/home</tt> partition, the backups should be written to the
<tt>/scratch/backups/cd/to-write/</tt> directory so they can be <a
href="howto.html#cd-r">written to a CD-R disk</a>.  All other
incremental dumps are ordinarily made to the <a
href="howto.html#zip">Zip drive</a>.  The Zip drive is <a
href="howto.html#zip-mount">mounted beforehand</a> if necessary, but not
<a href="howto.html#zip-unmount">unmounted afterward</a>.

<p>To see how many bytes are likely to be dumped, use the <tt>"-S"</tt>
option to <tt>dump</tt>, e.g.
<pre>
    dump -S2 /dev/hda9
</pre>
for a level 2 dump of the <tt>/dev/hda9</tt> partition (mounted on my
system as <tt>/home</tt>).

<p><b>Downloading:</b>

<p>The <a href="backup.pl"> <tt>backup.pl</tt> Perl script</a> (11793
bytes).  This script is free software; you may redistribute it and/or
modify it under the same terms as Perl itself.  Download this into
<tt>/root/bin/</tt>, make it executable, and you're ready to roll.

<a name='perl-script-bugs'>
<p><b>Known bugs:</b>
<ol>
  <li> Multi-volume backups can be a pain, especially to a Zip disk.  If
       the backup fills the volume, <tt>dump</tt> will stop and say
       <pre>
       DUMP: Change Volumes: Mount volume #2
       DUMP: Is the new volume mounted and ready to go?: ("yes" or "no") 
</pre>
       You then have to
       <ol>
	 <li> start up a separate shell;
	 <li> unmount the filled Zip disk;
	 <li> mount the new Zip disk (building a file system on it if
	      necessary); and
	 <li> go back to the first shell and respond, "Yes".
       </ol>
       And then the verification phase of the script doesn't work, since
       <tt>restore</tt> complains that the first volume is no longer
       mounted.  It is necessary to remount the first volume, type
       <pre>
       restore -C -y -f <i>dump-file-name</i>
</pre>
       and then replay the whole dance when <tt>restore</tt> asks for
       succeeding volumes.  -- rgr, 15-Jul-00.
  <li> Multivolume dumps to a directory are somewhat less painful,
       though still not automatic.  When dumping, the dump files must be
       renamed into new directories so that <tt>dump</tt> can put
       subsequent volumes in the same place.  I usually create
       subdirectories under <tt>/scratch/backups/</tt> with reasonable
       "volume names," e.g. <tt>2002Q1a</tt>, <tt>2002Q1b</tt>, etc.
       The verification must also be started manually, specifying each
       of the file names in their order of creation.  [But at least I
       don't have to keep swapping Zip disks in and out of the drive.
       -- rgr, 1-Feb-02.]
</ol>

<a name = script>
<h4>The <tt>backup</tt> shell script</h4>

The <tt>backup.sh</tt> shell script is the original version of the <a
href="#perl-script">Perl script</a>; it is less featureful, but
therefore simpler to use and modify, and defaults to <a
href="howto.html#zip">Zip drive</a> backups.  Usage for this is
<pre>
    backup [-test] [-cd] [<i>partition</i>] <i>dumplevel</i>
</pre>
where
<dl>
  <dt> <b>-test</b>
  <dd> causes the <tt>dump</tt> and <tt>restore</tt> commands to be
       echoed instead of executed.
  <dt> <b>-cd</b>
  <dd> causes the <tt>dump</tt> to be written to the
       <tt>/scratch/backups/</tt> directory, with a maximum volume size
       appropriate for 700MB CD-R disks.  Selecting a level 0 (complete)
       backup also forces the <tt>-cd</tt> option.  By default, backups
       are written to <tt>/mnt/zip/</tt> with a maximum volume size for
       a Zip100 disk.
  <dt> <b><i>partition</i></b>
  <dd> is the partition to be backed up, and defaults to
       <tt>/dev/hda9</tt> for the <tt>/home</tt> partition [on my disk,
       anyway].
  <dt> <b><i>dumplevel</i></b>
  <dd> is the backup level, a single digit.  0 is for a full backup, 1
       for most inclusive incremental, 9 for least inclusive incremental
       (<a href="#inc-levels">see below</a> for more).
</dl>

<p><b>Downloading:</b>

<p>The <a href="backup.sh"> <tt>backup.sh</tt> shell script</a> (3844
bytes).  I have placed this into the public domain, so you are free to
hack it any way you want.  Download this into <tt>/root/bin/</tt>, make
it executable, and you're ready to roll.

<p><b>Known bugs:</b>
<ol>
  <li> Multi-volume backups are are a pain, just as for the Perl script.
       For details, see <a href="#perl-script-bugs">the "bugs"
       section</a> of the Perl script writeup, above.
</ol>

<a name=strategy>
<h4>Backup strategy</h4>

<p>The current backup strategy is:
<ol>
  <li> Full backup (level 0) is done quarterly, on or shortly after the
       first of the quarter.  These are created manually, and are always
       written to CD.
  <li> Backups of the <tt>/home</tt> partition are created automatically
       every night via cron jobs, and then saved manually the next
       morning.  The relevant <tt>crontab</tt> entries look like this:
       <a name='cron-backup'>
       <pre>
# Backups.
# at 15 minutes after midnight every night, do a /home backup.
15 0 * * Mon	/root/bin/backup.pl -cd-dir /scratch/backups/cd/to-write /dev/hda9 3
15 0 * * Tue	/root/bin/backup.pl -cd-dir /scratch/backups/cd/to-write /dev/hda9 2
15 0 * * Wed	/root/bin/backup.pl -cd-dir /scratch/backups/cd/to-write /dev/hda9 5
15 0 * * Thu	/root/bin/backup.pl -cd-dir /scratch/backups/cd/to-write /dev/hda9 4
15 0 * * Fri	/root/bin/backup.pl -cd-dir /scratch/backups/cd/to-write /dev/hda9 7
15 0 * * Sat	/root/bin/backup.pl -cd-dir /scratch/backups/cd/to-write /dev/hda9 6
15 0 * * Sun	/root/bin/backup.pl -cd-dir /scratch/backups/cd/to-write /dev/hda9 1
</pre>
       Note how a consolidated backup (level 1) is done once a week, on
       Sunday morning, and the dailies are done according to <a
       href="#inc-levels">the backup schedule described below</a>.
  <li> Backups of other partitions are usually done manually on Sundays.
       Many partitions change only when upgraded, <tt>/boot</tt> and
       <tt>/usr</tt> in particular, so they don't always need to be
       backed up.  I usually skip a partition if the dump would be less
       than a megabyte.
  <li> Most dump files (all except for the <tt>/home</tt> backups noted
       above) are written to a Zip disk if small enough, else to a CD.
</ol>

<p>I try to schedule system upgrades and <a href="#mac-backup">Macintosh
file-copy backups</a> for the end of the month, just before the next
major backup, so the new files don't bloat the incrementals.  But this
is not as critical as it once was; now that I have <tt>/home</tt> on a
separate partition, I can keep my files on <tt>/home</tt> backed up and
leave whatever system upgrades I've applied in the mean time until the
weekend.

<a name = inc-levels>
<h4>Backup levels</h4>

The incrementals use the "modified Tower of Hanoi algorithm" described
in the <tt>dump</tt> manpage, which prescribes the following sequence of
dump levels:
<pre>
    3 2 5 4 7 6 9 8 9 9 ...
</pre>
These are for daily backups, which is the absolute minimum period for a
workgroup server in a "real" business installation.  Once I had the
process fully automated, I tried doing them daily, but that got to be
excessive; I don't generate as much stuff as an office full of people.
I still have the <a href="#cron-backup">relevant <tt>crontab</tt>
entries</a> in place for daily backups, but only copy the level 1, 2, 4,
and 6 dumps to other media.  Four dumps a week is plenty of coverage for
me, and copying even those manually is enough of a pain.  So I am
effectively back to doing backups four times a week.  I do keep the
level 3, 5, and 7 dumps around for a while on my <tt>/scratch</tt>
partition, which doesn't protect against a catastrophic disk hardware
failure, but would save me a day of lossage in case of irrecoverable
file system corruption on the <tt>/home</tt> partition.

<a name='backup-log'>
<h4>Backup log file</h4>

<p>I keep my backup log in <a
href="file:/home/rogers/linux/backup/backups.text">
<tt>~rogers/linux/backup/backups.text</tt></a>.  This file records what
backups were taken when, and what Zip disk or CD they live on.  I use it
to plan where to put the previous night's backup, as well as to decide
which old backups are no longer useful.  Since I do it regularly, the
whole process costs only five minutes or so of my time.  Most of the
work is deciding which disk to use; the actual copying of files usually
takes only a minute or two.

<p>[maybe should describe the format some day.  -- rgr, 8-Jan-03.]

<a name = tar-backups>
<h3>Making <tt>tar</tt> backups</h3>

<p>[This is not as versatile as <tt>dump</tt>, as incrementals are
harder, so I don't use it any more.  -- rgr, 1-Feb-02.]

<p>To do a complete <tt>tar</tt> backup (onto an existing Zip ext2fs
disk), use the <tt>~rogers/linux/backup/backup-home</tt> script, which
does <tt>"mount&nbsp;/mnt/zip"</tt> if needed and then creates a tar
file full backup of the <tt>~rogers/</tt> tree in the
<tt>/mnt/zip/backups/</tt> directory, comparing it when done.

<a name = mac-backup>
<h3>Backing up Jan's Macintosh</h3>

I try do this before monthly backups (but often forget to do it even
quarterly).  The basic idea is to use the Finder on the Mac to copy the
<tt>:Hard&nbsp;Disk:Documents:Jan:</tt> contents to Linux before it gets
backed up.  Then, extract only the files that have changed, so that
incrementals don't get bloated.
<ol>
  <li> Use the Finder to copy the <tt>:Documents:Jan:</tt> folder via
       the network to the <tt>~jan/backup/</tt> directory, where it will
       become <tt>~jan/backup/Jan/</tt>.
  <li> Blow away the Netscape cache in the <tt>~jan/backup/Jan/</tt>
       directory.  Something like
       <pre>
       cd ~jan/backup/
       rm -fr Jan/Netscape/Jan\ Rogers/Cache\ :c4
       </pre>
       should do the trick.
  <li> Use <tt>cp-if-newer.pl</tt> to copy changed files to
       <tt>~jan/backup/latest/</tt>:
       <pre>
       cp-if-newer.pl -pfR --dst-kludge Jan latest
       </pre>
       This will copy only those files that have changed, which makes
       less work for incremental backups.  Add the <tt>"-v"</tt> option
       to see what actually gets copied.  (And don't ask about
       <tt>--dst-kludge</tt>; you don't wanna know.)

       <p>[<tt>cp-if-newer.pl</tt> is more or less equivalent to doing
       the following with <tt>cp</tt>:
       <pre>
       cd ~jan/backup/Jan/
       cp -ufR . ../latest
       chown -R jan.jan ../latest
       </pre>
       except that <tt>.AppleDouble</tt> entries are created with the
       current date, so they are always copied even if unchanged.
       <tt>cp-if-newer.pl</tt> treats <tt>.AppleDouble</tt> directories
       specially, using the date of the "data fork" file in order to
       prevent this.  The source directory must be specified as
       <tt>"."</tt>, because <tt>cp</tt> always prepends this to the
       destination path.]

       <p>
  <li> <tt>~jan/backup/Jan/</tt> is no longer needed, so blow that
       away, too.
       <pre>
       rm -fr Jan
       </pre>
</ol>
Of course, I wouldn't need to do anything special if I could just train
Jan to use my Linux box as a file server . . .

<p>
<hr>
<address><a href="mailto:rogers@rgrjr.dyndns.org">Bob Rogers
	<tt>&lt;rogers@rgrjr.dyndns.org&gt;</tt></a></address>
<!-- hhmts start -->
Last modified: Sun Apr 27 21:19:22 EDT 2003
<!-- hhmts end -->
</body> </html>
