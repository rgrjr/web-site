<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML//EN">
<html> <head>
<title>Bob Rogers:  System backups</title>
</head>

<body bgcolor=white>
<h2>System backup</h2>

<p><a href="/"><tt>Home</tt></a> : <a href="index.html">Linux
resources</a> : <a href="howto.html">"Howto"</a> : Backup
<hr>

<h3>Table of contents</h3>

<!-- hhmtoc start -->
<ol>
  <li> System backup
       <ol>
	 <li> Table of contents
	 <li> <a href="#dump-vs-tar"><tt>dump</tt> vs. <tt>tar</tt></a>
	 <li> <a href="#dump-backups">Making backups with <tt>dump</tt></a>
	      <ol>
		<li> <a href="#perl-script">The <tt>backup.pl</tt> perl script</a>
		<li> <a href="#script">The <tt>backup</tt> shell script</a>
		<li> <a href="#strategy">Backup strategy</a>
		<li> <a href="#inc-levels">Backup levels</a>
		<li> <a href="#backup-log">Backup log file</a>
	      </ol>
	 <li> <a href="#tar-backups">Making <tt>tar</tt> backups</a>
	 <li> <a href="#mac-backup">Backing up Jan's Macintosh</a>
       </ol>
</ol>
<!-- hhmtoc end -->

<a name="dump-vs-tar">
<h3><tt>dump</tt> vs. <tt>tar</tt></h3>

<p>There are two "entry-level" backup options for Linux and Unix
systems in general:
<ol>
  <li> The standard Unix <tt>tar</tt> utility.  This is <a
       href="http://www.gnu.org/directory/GNU/tar.html"> GNU
       <tt>tar</tt></a> in free implementations (and even some other
       Unix flavors), and is the easiest tool for making complete
       backups of particular directories.
  <li> The traditional Unix <tt>dump</tt> and <tt>restore</tt> programs.
       Most Linux systems come with the <a
       href="http://dump.sourceforge.net/"> <tt>dump/restore</tt>
       implementation for ext2/ext3</a>, but these are the traditional
       names for the backup programs in Unix, so
       <tt>"man&nbsp;dump"</tt> will almost always come up with
       something on any Unix system.  <tt>dump</tt> can only operate on
       a whole partition at a time, but it features incremental backup
       capability, so you only need to back up what's changed.
</ol>

<p>For the most part, I use the standard, tried-and-true <tt>ext2</tt>
file system, and its successor <tt>ext3</tt>.  I have some experience
with <a href="http://www.namesys.com/">ReiserFS</a>, which appears to be
faster than <tt>ext2/3</tt> (and more reliable than <tt>ext2</tt>), but
still prefer <tt>ext3</tt>, because <a
href="http://dump.sourceforge.net/"> <tt>dump</tt></a> doesn't
understand ReiserFS, and there appears to be no good incremental backup
solution for ReiserFS.  (The word on the street is to use <a
href="http://www.gnu.org/directory/GNU/tar.html"> GNU <tt>tar</tt></a>
for Reiser partitions, but I've never been able to <tt>tar</tt> to make
incremental backups correctly.)  So, I use ReiserFS only for those
partitions I don't plan to back up regularly.

<p>Making the backup is only half of the problem; the dump file also
needs to be verified by comparing the contents with the original disk
data.  This is not as essential as it was in the bad old days, when
media were much less reliable, and hardware was much less clever about
dealing with media failures.  In fact, I've never yet found a corrupted
backup file during verification.  However, I've heard enough horror
stories of people not finding out that their backups were unreadable
until they needed them, so I don't consider the backup complete until
it's been verified in its final location.

<a name='dump-backups'>
<h3>Making backups with <tt>dump</tt></h3>

<p>[need a discussion of backup levels here?  -- rgr, 10-Jan-04.]

<p>Both of the following scripts call <tt>dump</tt> to create backup
files using the same naming convention, and then verify the file using
<tt>restore</tt>.  Initially, I wrote the <a href="#script">shell script
version</a> first, then recoded it into Perl when I needed to add
sufficient functionality that I found the shell syntax too confining.
The <a href="#perl-script"> <tt>backup.pl</tt> Perl script</a> has a
number of extra features, plus changed defaults that make it more
convenient to use for CD-ROM dumps.

<a name='perl-script'>
<h4>The <tt>backup.pl</tt> perl script</h4>

When <tt>backup.pl</tt> is run by root, it creates a <a
href="http://dump.sourceforge.net/"> <tt>dump</tt></a> format backup
file and verifies it using <tt>restore</tt>.  Usage for this is
<pre>
    backup.pl [--[no]cd] [--file-name=&lt;name&gt;] [--dump-dir=&lt;dest-dir&gt;]
              [--test] [--verbose] [--usage|-?] [--help] [--cd-dir=&lt;mv-dir&gt;]
              [--partition=&lt;block-special-device&gt;] [--level=&lt;digit&gt;]
              [--volsize=&lt;max-vol-size&gt;] [&lt;partition&gt;] [&lt;level&gt;]
</pre>
See <a href="backup.pl.html">the <tt>backup.pl</tt> <tt>man</tt>
page</a> for argument descriptions, and other details.

<p>For complete (level 0) dumps, as well as the larger level 1 dumps of
the <tt>/home</tt> partition, the backups should be written to the
<tt>/scratch/backups/cd/to-write/</tt> directory so they can be <a
href="howto.html#cd-r">written to a CD-R disk</a>.  All other
incremental dumps are ordinarily made to the <a
href="howto.html#zip">Zip drive</a>.  The Zip drive is <a
href="howto.html#zip-mount">mounted beforehand</a> if necessary, but not
<a href="howto.html#zip-unmount">unmounted afterward</a>.

<p>To see how many bytes are likely to be dumped, use the <tt>"-S"</tt>
option to <tt>dump</tt>, e.g.
<pre>
    dump -S2 /dev/hda9
</pre>
for a level 2 dump of the <tt>/dev/hda9</tt> partition (mounted on my
system as <tt>/home</tt>).

<p><b>Downloading:</b>

<p>The <a href="backup.pl"> <tt>backup.pl</tt> Perl script</a> (11793
bytes).  This script is free software; you may redistribute it and/or
modify it under the same terms as Perl itself.  Download this into
<tt>/root/bin/</tt>, make it executable, and you're ready to roll.

<a name='perl-script-bugs'>
<p><b>Known bugs:</b>
<ol>
  <li> Multi-volume backups can be a pain, especially to a Zip disk.  If
       the backup fills the volume, <tt>dump</tt> will stop and say
       <pre>
       DUMP: Change Volumes: Mount volume #2
       DUMP: Is the new volume mounted and ready to go?: ("yes" or "no") 
</pre>
       You then have to
       <ol>
	 <li> start up a separate shell;
	 <li> unmount the filled Zip disk;
	 <li> mount the new Zip disk (building a file system on it if
	      necessary); and
	 <li> go back to the first shell and respond, "Yes".
       </ol>
       And then the verification phase of the script doesn't work, since
       <tt>restore</tt> complains that the first volume is no longer
       mounted.  It is necessary to remount the first volume, type
       <pre>
       restore -C -y -f <i>dump-file-name</i>
</pre>
       and then replay the whole dance when <tt>restore</tt> asks for
       succeeding volumes.  -- rgr, 15-Jul-00.
  <li> Multivolume dumps to a directory are somewhat less painful,
       though still not automatic.  When dumping, the dump files must be
       renamed into new directories so that <tt>dump</tt> can put
       subsequent volumes in the same place.  I usually create
       subdirectories under <tt>/scratch/backups/</tt> with reasonable
       "volume names," e.g. <tt>2002Q1a</tt>, <tt>2002Q1b</tt>, etc.
       The verification must also be started manually, specifying each
       of the file names in their order of creation.  [But at least I
       don't have to keep swapping Zip disks in and out of the drive.
       -- rgr, 1-Feb-02.]
</ol>

<a name = script>
<h4>The <tt>backup</tt> shell script</h4>

The <tt>backup.sh</tt> shell script is the original version of the <a
href="#perl-script">Perl script</a>; it is less featureful, but
therefore simpler to use and modify, and defaults to <a
href="howto.html#zip">Zip drive</a> backups.  Usage for this is
<pre>
    backup [-test] [-cd] [<i>partition</i>] <i>dumplevel</i>
</pre>
where
<dl>
  <dt> <b>-test</b>
  <dd> causes the <tt>dump</tt> and <tt>restore</tt> commands to be
       echoed instead of executed.
  <dt> <b>-cd</b>
  <dd> causes the <tt>dump</tt> to be written to the
       <tt>/scratch/backups/</tt> directory, with a maximum volume size
       appropriate for 700MB CD-R disks.  Selecting a level 0 (complete)
       backup also forces the <tt>-cd</tt> option.  By default, backups
       are written to <tt>/mnt/zip/</tt> with a maximum volume size for
       a Zip100 disk.
  <dt> <b><i>partition</i></b>
  <dd> is the partition to be backed up, and defaults to
       <tt>/dev/hda9</tt> for the <tt>/home</tt> partition [on my disk,
       anyway].
  <dt> <b><i>dumplevel</i></b>
  <dd> is the backup level, a single digit.  0 is for a full backup, 1
       for most inclusive incremental, 9 for least inclusive incremental
       (<a href="#inc-levels">see below</a> for more).
</dl>

<p><b>Downloading:</b>

<p>The <a href="backup.sh"> <tt>backup.sh</tt> shell script</a> (3844
bytes).  I have placed this into the public domain, so you are free to
hack it any way you want.  Download this into <tt>/root/bin/</tt>, make
it executable, and you're ready to roll.

<p><b>Known bugs:</b>
<ol>
  <li> Multi-volume backups are are a pain, just as for the Perl script.
       For details, see <a href="#perl-script-bugs">the "bugs"
       section</a> of the Perl script writeup, above.
</ol>

<a name=strategy>
<h4>Backup strategy</h4>

<p>The basic strategy is as follows:
<ol>
  <li> Full backup (level 0) is done quarterly, usually on the first
       weekend of the quarter.  These are created manually, and written
       to CD.
  <li> Backups of the <tt>/home</tt> partition are created automatically
       every night via cron jobs.  The <tt>crontab</tt> entries for the
       full schedule look like this:
       <a name='cron-backup'>
       <pre>
# Backups.
# at 15 minutes after midnight every night, do a /home backup.
15 0 * * Mon	/root/bin/backup.pl -cd-dir /scratch/backups/cd/to-write /dev/hda9 3
15 0 * * Tue	/root/bin/backup.pl -cd-dir /scratch/backups/cd/to-write /dev/hda9 2
15 0 * * Wed	/root/bin/backup.pl -cd-dir /scratch/backups/cd/to-write /dev/hda9 5
15 0 * * Thu	/root/bin/backup.pl -cd-dir /scratch/backups/cd/to-write /dev/hda9 4
15 0 * * Fri	/root/bin/backup.pl -cd-dir /scratch/backups/cd/to-write /dev/hda9 7
15 0 * * Sat	/root/bin/backup.pl -cd-dir /scratch/backups/cd/to-write /dev/hda9 6
15 0 * * Sun	/root/bin/backup.pl -cd-dir /scratch/backups/cd/to-write /dev/hda9 1
</pre>
       Note how a consolidated backup (level 1) is done once a week, on
       Sunday morning, and the dailies are done according to <a
       href="#inc-levels">the backup schedule described below</a>.
  <li> The actual home schedule omits the Monday, Wednesday, and Friday
       backups, so I move the dump files to CD on Tuesday, Thursday,
       Saturday, and Sunday.
  <li> The backup schedule at work is similar, except that I do a level
       1 backup on Saturday, and nothing on Sunday.  Backups of the main
       RAID partitions are made to a separate disk, and then copied to
       another system via the <tt>vacuum.pl</tt> script.  [need link.
       -- rgr, 11-Jan-04.]
  <li> Backups of other partitions are usually done manually on Sundays.
       Many partitions change only when upgraded, <tt>/boot</tt> and
       <tt>/usr</tt> in particular, so they don't always need to be
       backed up.  I usually skip a partition if the dump would be less
       than a megabyte.
  <li> All dump files are written to CD using the <tt>cd-dump.pl</tt>
       script.  [need link.  -- rgr, 11-Jan-04.]  (I used to write the
       small ones to a Zip disk, but my Zip performance went south when
       I upgraded to SuSE 8.1, so that became too time-consuming.)
</ol>

<p>I try to schedule system upgrades and <a href="#mac-backup">Macintosh
file-copy backups</a> for the end of the month, just before the next
major backup, so the new files don't bloat the incrementals.  But this
is not as critical as it once was; now that I have <tt>/home</tt> on a
separate partition, I can keep my files on <tt>/home</tt> backed up and
leave whatever system upgrades I've applied in the mean time until the
weekend.

<a name = inc-levels>
<h4>Backup levels</h4>

The incrementals use the "modified Tower of Hanoi algorithm" described
in the <tt>dump</tt> manpage, which prescribes the following sequence of
dump levels:
<pre>
    3 2 5 4 7 6 9 8 9 9 ...
</pre>
These are for daily backups, which is the absolute minimum period for a
workgroup server in an office environment.  Once I had the process fully
automated, I tried doing them daily, but that got to be excessive; I
don't generate as much stuff as an office full of people.  Consequently,
I only do the level 1, 2, 4, and 6 dumps in the <a href="#cron-backup">
<tt>crontab</tt> schedule</a> above.  Four dumps a week is plenty of
coverage for me -- and copying even those manually keeps be busy enough.

<a name='backup-log'>
<h4>Backup log file</h4>

<p>I keep my backup log in <a
href="file:/home/rogers/linux/backup/backups.text">
<tt>~rogers/linux/backup/backups.text</tt></a>.  This file records what
backups were taken when, and what Zip disk or CD they live on.  I use it
to plan where to put the previous night's backup, as well as to decide
which old backups are no longer useful.  Since I do it regularly, the
whole process costs only five minutes or so of my time.  Most of the
work is deciding which disk to use; the actual copying of files usually
takes only a minute or two.

<p>[maybe should describe the format some day.  -- rgr, 8-Jan-03.]

<a name = tar-backups>
<h3>Making <tt>tar</tt> backups</h3>

<p>To do a complete <tt>tar</tt> backup (onto an existing Zip ext2fs
disk), use the <tt>~rogers/linux/backup/backup-home</tt> script, which
does <tt>"mount&nbsp;/mnt/zip"</tt> if needed and then creates a tar
file full backup of the <tt>~rogers/</tt> tree in the
<tt>/mnt/zip/backups/</tt> directory, comparing it when done.

<a name = mac-backup>
<h3>Backing up Jan's Macintosh</h3>

I try do this before monthly backups (but often forget to do it even
quarterly).  The basic idea is to use the Finder on the Mac to copy the
<tt>:Hard&nbsp;Disk:Documents:Jan:</tt> contents to Linux before it gets
backed up.  Then, extract only the files that have changed, so that
incrementals don't get bloated.
<ol>
  <li> Use the Finder to copy the <tt>:Documents:Jan:</tt> folder via
       the network to the <tt>~jan/backup/</tt> directory, where it will
       become <tt>~jan/backup/Jan/</tt>.
  <li> Blow away the Netscape cache in the <tt>~jan/backup/Jan/</tt>
       directory.  Something like
       <pre>
       cd ~jan/backup/
       rm -fr Jan/Netscape/Jan\ Rogers/Cache\ :c4
       </pre>
       should do the trick.
  <li> Use <tt>cp-if-newer.pl</tt> to copy changed files to
       <tt>~jan/backup/latest/</tt>:
       <pre>
       cp-if-newer.pl -pfR --dst-kludge Jan latest
       </pre>
       This will copy only those files that have changed, which makes
       less work for incremental backups.  Add the <tt>"-v"</tt> option
       to see what actually gets copied.  (And don't ask about
       <tt>--dst-kludge</tt>; you don't wanna know.)

       <p>[<tt>cp-if-newer.pl</tt> is more or less equivalent to doing
       the following with <tt>cp</tt>:
       <pre>
       cd ~jan/backup/Jan/
       cp -ufR . ../latest
       chown -R jan.jan ../latest
       </pre>
       except that <tt>.AppleDouble</tt> entries are created with the
       current date, so they are always copied even if unchanged.
       <tt>cp-if-newer.pl</tt> treats <tt>.AppleDouble</tt> directories
       specially, using the date of the "data fork" file in order to
       prevent this.  The source directory must be specified as
       <tt>"."</tt>, because <tt>cp</tt> always prepends this to the
       destination path.]

       <p>
  <li> <tt>~jan/backup/Jan/</tt> is no longer needed, so blow that
       away, too.
       <pre>
       rm -fr Jan
       </pre>
</ol>
Of course, I wouldn't need to do anything special if I could just train
Jan to use my Linux box as a file server . . .

<p>
<hr>
<address><a href="mailto:rogers@rgrjr.dyndns.org">Bob Rogers
	<tt>&lt;rogers@rgrjr.dyndns.org&gt;</tt></a></address>
<!-- hhmts start -->
Last modified: Sun Jan 11 13:48:47 EST 2004
<!-- hhmts end -->
</body> </html>
